{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import itertools\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from spellchecker import SpellChecker\n",
    "import html\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras.layers import Input,Dense,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.layers import Flatten,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D\n",
    "from keras.models import Model,Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = 'nlp-getting-started/'\n",
    "test_df = pd.read_csv(os.path.join(path_to_dataset, 'test.csv'))\n",
    "train_df = pd.read_csv(os.path.join(path_to_dataset, 'train.csv'))\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertTknzr = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_wordlist = list(bertTknzr.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format('word2vec_twitter_tokens.bin', binary=True, unicode_errors='ignore') \n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('word2vec_twitter_tokens.bin',binary=Trueï¼Œunicode_errors='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell Check functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_length(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    spell = SpellChecker()\n",
    "    reduced_text  = list([reduce_length(word) for word in text])\n",
    "    corrected_text = [spell.correction(word) for word in reduced_text]\n",
    "    print(reduced_text)\n",
    "    misspelled = spell.unknown(reduced_text)\n",
    "    print(misspelled)\n",
    "    for word in misspelled:\n",
    "        # Get the one `most likely` answer\n",
    "        print(spell.correction(word))\n",
    "        # Get a list of `likely` options\n",
    "        print(spell.candidates(word))\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def html_unescape(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "#tokenize sentence and find emoji stands for happy and sad\n",
    "def token(text):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    words = [word for word in tknzr.tokenize(text)]\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in word2vec_model:\n",
    "            if word2vec_model.similarity(words[i],\":)\")>0.55:\n",
    "                words[i] = 'happy'\n",
    "            elif word2vec_model.similarity(words[i],\":(\")>0.55:\n",
    "                words[i] = 'sad'\n",
    "    #return words\n",
    "    return ' '.join(words)\n",
    "\n",
    "def reduce_length(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    spell = SpellChecker()\n",
    "    text = text.split()\n",
    "    reduced_text  = [reduce_length(word) for word in text]\n",
    "    corrected_text = [word if (word in bert_wordlist) else spell.correction(word) for word in reduced_text]\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "def remove_digits(text): \n",
    "    pattern = '[0-9]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def textClean(df):\n",
    "    df['text'] = df['text'].apply(lambda text: html.unescape(text))\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    df['text'] = df['text'].apply(lambda text: remove_URL(text))\n",
    "    df['text'] = df['text'].apply(lambda text: remove_digits(text))\n",
    "    df['text'] = df['text'].apply(lambda text: remove_html(text))\n",
    "    df['text'] = df['text'].apply(lambda text: token(text))\n",
    "    df['text'] = df['text'].apply(lambda text: remove_punc(text))\n",
    "    for i in tqdm(range(len(df['text']))):\n",
    "        df['text'][i] = correct_spelling(df['text'][i])\n",
    "    #df['text'] = df['text'].apply(lambda text: correct_spelling(text))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d745741df5234455b7ab17fea4895e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7613.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_train = textClean(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('clean_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8813e458f747d69abc3922b0633771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3263.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_test = textClean(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('clean_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
